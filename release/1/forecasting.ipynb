{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "\n",
    "by Bart De Vylder and Pieter Buteneers from CoScale\n",
    "\n",
    "\n",
    "## Imports\n",
    "\n",
    "Let's first start with importing all the necessary packages. Some imports will be repeated in the exercises but if you want to skip some parts you can just execute the imports below and start with any exercise.\n",
    "\n",
    "As you can see we also import packages from `__future__`. This is to improve the compatibility with Python 3, but will not guarantee it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (13.0, 8.0)\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle_helper\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "import sklearn.gaussian_process\n",
    "import sklearn.ensemble\n",
    "\n",
    "# to make the code is compatible with python 3\n",
    "from __future__ import print_function   # turns print into a function\n",
    "from __future__ import division         # makes sure 3/2 = 1.5 and not 1 (use 3//2 = 1 instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the forecasting we are going to use page views data, very similar to the data used in the anomaly detection section. It is also page view data and contains 1 sample per hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data = pickle_helper.load('data/train_set_forecasting.pickle')\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(ts_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above you can clearly see that there is seasonality and a rising trend in the data.\n",
    "\n",
    "## One-step ahead prediction\n",
    "\n",
    "This forecasting section will describe the one-step ahead prediction. This means in this case that we will only predict the next data point which is in this case the number of pageviews in the next hour. \n",
    "\n",
    "As input for the prediction of point $x_{t+1}$ we use a limited history of size ```width``` ($w$) of the known points:\n",
    "\n",
    "$$x_{t-w},\\ldots  x_{t-2},  x_{t-1}, x_t \\rightarrow x_{t+1} $$\n",
    "\n",
    "\n",
    "In order to train a model on a timeseries we first convert in into a the training format sklearn accepts:\n",
    "\n",
    "```\n",
    "    x_train = [[x_0, x_1, x_2, ...],          y_train = [x_w,\n",
    "               [x_1, x_2, x_3, ...],                     x_{w+1},  \n",
    "                                ...]                          ...]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train a simple linear regression model on this training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "36dfa196c906bea674037480f40a5092",
     "grade": false,
     "grade_id": "convert",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_time_series_to_train_data(ts, width):\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(len(ts) - width - 1):\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "#         x_train.append( ? )\n",
    "#         y_train.append( ? )\n",
    "    return np.array(x_train), np.array(y_train)\n",
    "\n",
    "\n",
    "# some tests\n",
    "width = 5\n",
    "x_train, y_train = convert_time_series_to_train_data(ts_data, width)\n",
    "\n",
    "assert x_train.shape == (303, width)\n",
    "assert y_train.shape == (303,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 5\n",
    "x_train, y_train = convert_time_series_to_train_data(ts_data, width)\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "print('The R2 score of the linear model with width =', width, 'is', model.score(x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to visualize our prediction by repeating the one-step-ahead prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "3d8e9003e1623a67670157514891b1fb",
     "grade": false,
     "grade_id": "predict",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# this is a helper function to make the predictions\n",
    "def predict(model, ts_data, width, nof_points):\n",
    "    prediction = []\n",
    "    # create the input data set for the first predicted output\n",
    "    # copy the data to make sure the original is not overwritten\n",
    "    x_test = np.copy(ts_data[-width : ])\n",
    "    for i in range(nof_points):\n",
    "        # predict only the next data point\n",
    "        prediction.append(model.predict(x_test.reshape((1, -1)))[0])\n",
    "        # use the newly predicted data point as input for the next prediction\n",
    "        x_test[0 : -1] = x_test[1 : ]\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "        # x_test[-1] = ?\n",
    "\n",
    "    return np.array(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nof_predictions = 200\n",
    "prediction = predict(model, ts_data, width, nof_predictions)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(np.concatenate((ts_data, prediction)), 'g')\n",
    "plt.plot(ts_data, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the image above the prediction is not very good. Let's play a bit with the ```width``` variable below to see if you can find a better forecast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e919f3a56fef96390041c92ff28a6911",
     "grade": false,
     "grade_id": "jkl",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# width = ?\n",
    "\n",
    "x_train, y_train = convert_time_series_to_train_data(ts_data, width)\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "print('The R2 score of the linear model with width =', width, 'is', model.score(x_train, y_train))\n",
    "\n",
    "prediction = predict(model, ts_data, width, 200)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(np.concatenate((ts_data, prediction)), 'g')\n",
    "plt.plot(ts_data, 'b')\n",
    "plt.show()\n",
    "\n",
    "assert width > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well you found that in this case the prediction task performs better if the model receives more data. In particular, due to the seasonality the value of the day before is an important part of the prediction. This we can also verify by inspecting the coefficients assigned by the linear method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.coef_)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(range(-width+1, 1),  model.coef_)\n",
    "plt.xlabel(\"lag\")\n",
    "plt.ylabel(\"coefficient for prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the same thing for the following models:\n",
    "* ```sklearn.linear_model.RidgeCV()```\n",
    "* ```sklearn.linear_model.LassoCV()```\n",
    "\n",
    "Both models also estimate the noise that is present in the data to avoid overfitting. `RidgeCV()` will keep the weights that are found small, but it won't put them to zero. `LassoCV()` on the other hand will put several weights to 0 (inspect the ```model.coef_``` to verify). \n",
    "\n",
    "## Automation\n",
    "\n",
    "What we have done up to now is manually selecting the best outcome based on the test result. This can be considered cheating because you have just created a self-fulfilling prophecy. Additionally it is not only cheating it is also hard to find the exact `width` that gives the best result by just visually inspecting it. So we need a more objective approach to solve this.\n",
    "\n",
    "To automate this process you can use a validation set. In this case we will use the last 48 hours of the training set to validate the score and select the best parameter value. This means that we will have to use a subset of the training set to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "97dcf5e7fa80a0759743c209c33ac848",
     "grade": false,
     "grade_id": "find_best_model",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "model_generators = [sklearn.linear_model.LinearRegression, sklearn.linear_model.RidgeCV,\n",
    "                    sklearn.linear_model.LassoCV]\n",
    "best_score = 0\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# for model_gen in ? :\n",
    "#    for width in range( ? , ? ): \n",
    "        x_train, y_train = convert_time_series_to_train_data(ts_data, width)\n",
    "        # train the model on the first 48 hours\n",
    "        x_train_i, y_train_i = x_train[ : -48, :], y_train[ : -48]\n",
    "        # use the last 48 hours for validation\n",
    "        x_val_i, y_val_i = x_train[-48 : ], y_train[-48 : ]\n",
    "        \n",
    "        ##### Implement this part of the code #####\n",
    "        raise NotImplementedError()\n",
    "        # model = \n",
    "        \n",
    "        # there is a try except clause here because some models do not converge for some data\n",
    "        try:\n",
    "            ##### Implement this part of the code #####\n",
    "            raise NotImplementedError()\n",
    "            # model.fit( ? , ? )\n",
    "            # this_score = ?\n",
    "            \n",
    "            if this_score > best_score:\n",
    "                best_score = this_score\n",
    "                best_model_gen = model_gen\n",
    "                best_width = width\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(best_model_gen().__class__, 'was selected as the best model with a width of', best_width,\n",
    "      'and a validation R2 score of', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct the LassoCV methods was selected.\n",
    "\n",
    "Now we are going to train this best model on all the data. In this way we use all the available data to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f9583c73ff526d679ddb3bd7ca53a2f5",
     "grade": false,
     "grade_id": "best_model_gen_plot",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# width = ?\n",
    "# model = ?\n",
    "\n",
    "x_train, y_train = convert_time_series_to_train_data(ts_data, width)\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# model.fit( ? , ? )\n",
    "\n",
    "nof_predictions = 200\n",
    "prediction = predict(model, ts_data, width, nof_predictions)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(np.concatenate((ts_data, prediction)), 'g')\n",
    "plt.plot(ts_data, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altough the optimal result found here might not be the best visually, it is a far better result than the one you selected manually just because there was no cheating involved ;-).\n",
    "\n",
    "Some additional info:\n",
    "* This noise level of `RidgeCV()` and `LassoCV()` is estimated by automatically performing train and validation within the method itself. This will make them much more robust against over-fitting. The actual method used is [Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) which is a better approach of what we do here because it repeats the training and validation multiple times for different training and validation sets. The parameter that is set for these methods is often called the regularization parameter in literature and is well suited to avoid over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
